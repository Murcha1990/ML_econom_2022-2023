{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gStgBJy2WhAx"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1hqb2oWg96"
      },
      "source": [
        "# Семинар 7. Инструменты для работы с языком"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXhKw6r2Wg97"
      },
      "source": [
        "... или зачем нужна предобработка.\n",
        "\n",
        "Раньше мы смотрели на светлую сторону анализа данных - построение моделей. Теперь попробуем глубже посмотреть на часть про предобработку данных. Задача предобработки особенно актуальна, если мы имеем дело с текстами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOWdHcUWg98"
      },
      "source": [
        "## Задача: классификация твитов по тональности\n",
        "\n",
        "У нас есть выборка из твитов.\n",
        "Нам известна эмоциональная окраска каждого твита из выборки: положительная или отрицательная. Задача состоит в построении модели, которая по тексту твита предсказывает его эмоциональную окраску.\n",
        "\n",
        "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "Скачиваем выборку ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYvTkhGnWg98"
      },
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно скачать данные так:\n",
        "%%capture\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX-AeH8RWg9-"
      },
      "source": [
        "import pandas as pd # библиотека для удобной работы с датафреймами\n",
        "import numpy as np # библиотека для удобной работы со списками и матрицами\n",
        " \n",
        "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtGDKFvQYEDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f32dd9f-1f90-4193-8b7c-3a594c0af205"
      },
      "source": [
        "!head positive.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"408906692374446080\";\"1386325927\";\"pleease_shut_up\";\"@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)\";\"1\";\"0\";\"0\";\"0\";\"7569\";\"62\";\"61\";\"0\"\r\n",
            "\"408906692693221377\";\"1386325927\";\"alinakirpicheva\";\"Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D\";\"1\";\"0\";\"0\";\"0\";\"11825\";\"59\";\"31\";\"2\"\r\n",
            "\"408906695083954177\";\"1386325927\";\"EvgeshaRe\";\"RT @KatiaCheh: Ну ты идиотка) я испугалась за тебя!!!\";\"1\";\"0\";\"1\";\"0\";\"1273\";\"26\";\"27\";\"0\"\r\n",
            "\"408906695356973056\";\"1386325927\";\"ikonnikova_21\";\"RT @digger2912: \"\"Кто то в углу сидит и погибает от голода, а мы ещё 2 порции взяли, хотя уже и так жрать не хотим\"\" :DD http://t.co/GqG6iuE2…\";\"1\";\"0\";\"1\";\"0\";\"1549\";\"19\";\"17\";\"0\"\r\n",
            "\"408906761416867842\";\"1386325943\";\"JumpyAlex\";\"@irina_dyshkant Вот что значит страшилка :D\n",
            "Но блин,посмотрев все части,у тебя создастся ощущение,что авторы курили что-то :D\";\"1\";\"0\";\"0\";\"0\";\"597\";\"16\";\"23\";\"1\"\r\n",
            "\"408906761769598976\";\"1386325943\";\"JustinB94262583\";\"ну любишь или нет? — Я не знаю кто ты бля:D http://t.co/brf9eNg1U6\";\"1\";\"0\";\"0\";\"0\";\"40\";\"6\";\"16\";\"0\"\r\n",
            "\"408906762436481024\";\"1386325943\";\"twinkleAYO\";\"RT @SpoonLamer: Ох,900 :D ну это конечно же @twinkleAYO . Чтобы у нее было много друзей, ведь она такая мимими &lt;3\";\"1\";\"0\";\"1\";\"0\";\"5169\";\"58\";\"43\";\"2\"\r\n",
            "\"408906764114206720\";\"1386325944\";\"pycalyruhog\";\"RT @veregijytaqo: У тебя есть ухажёр? Нет - мои уши не кто не жрёт :D\";\"1\";\"0\";\"2\";\"0\";\"393\";\"112\";\"153\";\"0\"\r\n",
            "\"408906764608749568\";\"1386325944\";\"grishintv\";\"Поприветствуем моего нового читателя @Alexey1789 ;)\";\"1\";\"0\";\"0\";\"0\";\"5872\";\"1387\";\"1431\";\"12\"\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVsSSqFKbAWL"
      },
      "source": [
        "Откроем файлы и создадим массив из текстов и правильных меток для твитов.\n",
        "Сначала идут положительные твиты, потом отрицательные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBrSNCKVWg9_"
      },
      "source": [
        "# загружаем положительные твиты\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
        " \n",
        "# загружаем отрицательные твиты\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
        " \n",
        "# соединяем вместе\n",
        "df = positive.append(negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfjWsULtaGtf"
      },
      "source": [
        "Посмотрим на полученные данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqudQEvUWg-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "38ce5973-7d39-4f43-c778-b9aad72bad9b"
      },
      "source": [
        "df.sample(5, random_state=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text     label\n",
              "15931   RT @Blawar_1337: Теперь у нас с @Wake_UA появи...  positive\n",
              "59532   с днём рождения зайка*))) ухх погуляем мы сего...  positive\n",
              "47185   RT @Shumkova0406199: @ann_safina Вов вов вов А...  negative\n",
              "42002   Надо выдернуть звуковую дорожку из \"Доктора Ка...  positive\n",
              "109035  @_hassliebe_ может все таки на этой неделе вер...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61574dbb-05ec-4c37-9c10-725d95214750\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15931</th>\n",
              "      <td>RT @Blawar_1337: Теперь у нас с @Wake_UA появи...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59532</th>\n",
              "      <td>с днём рождения зайка*))) ухх погуляем мы сего...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47185</th>\n",
              "      <td>RT @Shumkova0406199: @ann_safina Вов вов вов А...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42002</th>\n",
              "      <td>Надо выдернуть звуковую дорожку из \"Доктора Ка...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109035</th>\n",
              "      <td>@_hassliebe_ может все таки на этой неделе вер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61574dbb-05ec-4c37-9c10-725d95214750')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61574dbb-05ec-4c37-9c10-725d95214750 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61574dbb-05ec-4c37-9c10-725d95214750');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubV1-2JjaZuC"
      },
      "source": [
        "Разбиваем данные на обучающую и тестовую выборки с помощью функции ```train_test_split()``` из **sklearn**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RscvM_TWg-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64974685-3988-4f17-df11-351dc2b4f27d"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)\n",
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94011       RT @Mr_Malcev: Пипец,завтро с первой смены+(((\n",
              "51072    @DaniilPanchenko везет..а нам еще неделю ебаться(\n",
              "75700    RT @pyxavohaxuz: Пытаюсь запустить флеш под ва...\n",
              "23303    RT @toklohcteI: Soon или soon? \\nНе можем выбр...\n",
              "24594    @AsemEreje дұрыс айтасыз, жақсы жер деп сол үш...\n",
              "                               ...                        \n",
              "55728    кто спёр мой транспортир?:D\\nнет уже 4 транспо...\n",
              "69294    @vetrova_arina бляяя,  ну почему сука я не зна...\n",
              "86248          Абсолютно не умею распоряжаться временем(((\n",
              "88938    Ну вот, из– за этого не иду на английский :(\\n...\n",
              "92807    @Leysanlubimka это студенческий фольклер 70-х,...\n",
              "Name: text, Length: 170125, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpq4QOU5Wg-H"
      },
      "source": [
        "## Baseline: классификация необработанных n-грамм\n",
        "\n",
        "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения, такой как логистическая регрессия. \n",
        "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
        "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7DyyXRWg-K"
      },
      "source": [
        "## Что такое n-граммы:\n",
        "\n",
        "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
        "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiUoyqNb3WA"
      },
      "source": [
        "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcrWxBzzWg-K"
      },
      "source": [
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ib-zYTvfQq5"
      },
      "source": [
        "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Ql8Em4Wg-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280b0ba2-6e25-4fa9-e62a-9691d9760f19"
      },
      "source": [
        "sentence = 'Если б мне платили каждый раз'.split()\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6V5P2Jcc4Oy"
      },
      "source": [
        "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
        "\n",
        "На вход передается два параметра:\n",
        "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
        "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
        "\n",
        "\n",
        "Чтобы полученный объект отобразить, делаем из него ```list```. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oqpykUc5e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0992f744-6d5e-468e-9fc4-e66ceaa359ec"
      },
      "source": [
        "list(ngrams(sentence, 1)) # униграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKRhRlxfoj4"
      },
      "source": [
        "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzl6t5dpWg-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ef0dc5-9ef7-4cce-c579-c2ab5b7dfcea"
      },
      "source": [
        "list(ngrams(sentence, 2)) # биграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCkkFzWLWg-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea85fba-4560-493f-c09d-1d291ad78a53"
      },
      "source": [
        "list(ngrams(sentence, 3)) # триграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygS6_fJWg-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3761bc3b-b530-4031-b0db-3147d231f3c5"
      },
      "source": [
        "list(ngrams(sentence, 5)) # ... пентаграммы?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JewKs4XU-so"
      },
      "source": [
        "### Векторизаторы\n",
        "\n",
        "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
        "\n",
        "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hiNv2eVAc-"
      },
      "source": [
        "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в знакомой нам библиотеке **sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPplZnxeVEBR"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\", см. далее"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBN16KYZWg-U"
      },
      "source": [
        "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает следующую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklbwY_vWg-X"
      },
      "source": [
        "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJmHbOvVmln"
      },
      "source": [
        "<a href=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"800\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLTEzNWxrx5X"
      },
      "source": [
        "Инициализируем ```CountVectorizer()```, указав в качестве признаков униграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKQcLfBWg-W"
      },
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-gFUNm6huAz"
      },
      "source": [
        "После инициализации _vectorizer_ можно обучить на наших данных. \n",
        "\n",
        "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с label encoderом и one-hot-encoderом.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJESwGfhq4v"
      },
      "source": [
        "vectorized_x_train = vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbIKYfgyCHT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904834fe-4673-49f2-cf59-f44b40935d2d"
      },
      "source": [
        "vectorized_x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<170125x243617 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1847960 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJDPt7xvWKe"
      },
      "source": [
        "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyT1Kz6Ppt8n"
      },
      "source": [
        "В vectorizer.vocabulary_ лежит словарь, отображение слов в их индексы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72knqTvCWg-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf0e7b3-9f14-4c16-94cb-9791f7472fca"
      },
      "source": [
        "list(vectorizer.vocabulary_.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rt', 74597),\n",
              " ('mr_malcev', 59533),\n",
              " ('пипец', 182397),\n",
              " ('завтро', 133145),\n",
              " ('первой', 179930),\n",
              " ('смены', 211532),\n",
              " ('daniilpanchenko', 23085),\n",
              " ('везет', 110835),\n",
              " ('нам', 163299),\n",
              " ('еще', 130686)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdKqMyRKjA-p"
      },
      "source": [
        "В нашей выборке 170125 текстов (твитов), в них встречается 243760 разных слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjOD7nrsi2fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8412e265-521e-47be-8a16-af5be3e16bae"
      },
      "source": [
        "vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170125, 243617)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7geX_YHDjG9v"
      },
      "source": [
        "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить модель логистической регрессии (или любую другую из тех, на которые мы смотрели раньше, например, случайный лес)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCdROyxsWg-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40eae481-3574-49b3-808e-27b281930770"
      },
      "source": [
        "clf = LogisticRegression(random_state=42, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
        "clf.fit(vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z9iIlzOjVUF"
      },
      "source": [
        "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
        "\n",
        "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2_AW8FpjWHQ"
      },
      "source": [
        "vectorized_x_test = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahCRPAeWjtcl"
      },
      "source": [
        "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
        "\n",
        "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvxbzinWg-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e10c88a-9bf4-474d-f45c-8da0a91238c6"
      },
      "source": [
        "pred = clf.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.77      0.76     27831\n",
            "    positive       0.77      0.76      0.77     28878\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yiLk1P_xYQ2"
      },
      "source": [
        "## N-граммы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjy5ZPmwWg-j"
      },
      "source": [
        "Попробуем сделать то же самое, используя в качестве признаков триграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQHqUpRWg-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861c6ce8-6d05-44c0-b3ae-eee36438cba4"
      },
      "source": [
        "# инициализируем векторайзер \n",
        "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(trigram_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторизатор к тестовым данным\n",
        "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(trigram_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.85      0.68     27831\n",
            "    positive       0.72      0.38      0.50     28878\n",
            "\n",
            "    accuracy                           0.61     56709\n",
            "   macro avg       0.65      0.62      0.59     56709\n",
            "weighted avg       0.65      0.61      0.59     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MonLW7AyWg-m"
      },
      "source": [
        "Как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWxW3e9Wg-m"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hCxZRtWg-m"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "**TF (term frequency)** – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
        "\n",
        "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Сакральный смысл: если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7DfTkJWg-n"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f_zZm14PHM"
      },
      "source": [
        "Действуем аналогично, как с ```CountVectorizer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rMYiobAWg-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43edfdae-88c6-4670-f50a-6ec6b5f47f95"
      },
      "source": [
        "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторизатор к тестовым данным\n",
        "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.78      0.75     27831\n",
            "    positive       0.77      0.71      0.74     28878\n",
            "\n",
            "    accuracy                           0.74     56709\n",
            "   macro avg       0.75      0.74      0.74     56709\n",
            "weighted avg       0.75      0.74      0.74     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkuods5LWg-q"
      },
      "source": [
        "В этот раз получилось хуже :( Вернёмся к `CountVectorizer()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39SSh0zWg-r"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
        "\n",
        "Самый наивный способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSe08N2Wg-r"
      },
      "source": [
        "import nltk # уже знакомая нам библиотека nltk\n",
        "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiDt3L8Y8god"
      },
      "source": [
        "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому, особо не задумываясь, запустите следующую ячейку:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPH3yMcumsdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a17fe98-5086-4d2d-d44d-b74bfa861573"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfDb8D_9DqD"
      },
      "source": [
        "Применим токенизацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrJDGpgYWg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e3ccd4-8748-4f59-c12e-1989dd9feb1a"
      },
      "source": [
        "example = 'Но не каждый хочет что-то исправлять:('\n",
        "word_tokenize(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxy7KGZI9bhK"
      },
      "source": [
        "Если использовать просто ```split()```, то грустный смайлик :( не отделяется от слова \"исправлять\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52dIuSI9W6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8a0145-202d-4cbd-dd31-e5416691d9d9"
      },
      "source": [
        "example.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8oPYoTWg-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1931187-69c4-403b-b114-5c798119aa74"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnGCL5iWg-8"
      },
      "source": [
        "Они умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jejj5X7QWg-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473ce35d-c70d-4949-b27b-0876e42e0c8f"
      },
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(example))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-wf6A1EWg--"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2REwpHGWWg-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b5ec5d-056f-4eb9-f9f8-f743abc59a89"
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tckre90JWg_B"
      },
      "source": [
        "А некоторые -- вообще не для текста на естественном языке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Ml3xtaWg_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be1c971-a9c1-428b-b012-f39031bd8a8e"
      },
      "source": [
        "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(a (b c))', 'd', 'e', '(f)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2kvAo0_b93"
      },
      "source": [
        "**Правильный токенизатор подбирается исходя из требований задачи!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVrgkSaWg_K"
      },
      "source": [
        "## Стоп-слова и пунктуация\n",
        "\n",
        "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-h6WKyWg_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08cb7037-3af0-436b-e74e-f8ea811200c2"
      },
      "source": [
        "# импортируем стоп-слова из библиотеки nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# посмотрим на стоп-слова для русского языка\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihn2Y_SzBU57"
      },
      "source": [
        "*Знаки* пунктуации лучше импортировать из модуля **String**. В нем хранятся различные наборы констант для работы со строками (пунктуация, алфавит и др.). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64U3FdPWg_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fe7dbcb8-46d8-491f-9006-9c7ff93d7240"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9LGTLaEBwsC"
      },
      "source": [
        "Объединим стоп-слова и знаки пунктуации вместе и запишем в переменную ```noise```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfH5y50wWg_O"
      },
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3gweXaWWg_P"
      },
      "source": [
        "Теперь нужно обучать нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
        "\n",
        "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
        "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
        "* какой токенизатор мы используем, параметр **tokenizer**;\n",
        "* какие у нас стоп-слова, параметр **stop_words**.\n",
        "\n",
        "*Напоминание:* мы используем готовый токенизатор ```word_tokenize```, а стоп-слова хранятся в переменной ```noise```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbXrVeRRuAxx"
      },
      "source": [
        "# инициализируем умный векторайзер \n",
        "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
        "                                   tokenizer=word_tokenize, \n",
        "                                   stop_words=noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nc6D-nwWg_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c46a805-ec61-4049-f1e4-1d66aceba6b3"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(smart_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(smart_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.80      0.78     28023\n",
            "    positive       0.79      0.76      0.78     28686\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWB1foQWg_T"
      },
      "source": [
        "Получилось лучше: accuracy выше, а также заметно подрос recall у негативного класса. \n",
        "\n",
        "Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRf9T_SWg_U"
      },
      "source": [
        "## Бонус*: Лемматизация\n",
        "\n",
        "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**. Почему это хорошо?\n",
        "* Во-первых, естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть хороших лемматизатор pymorphy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKZG2MwWg_f"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYWYq4BzOon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a55609-37f9-4268-9b5e-4fb47cc96fbb"
      },
      "source": [
        "# устанавливаем pymorphy2\n",
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 7.4MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdT2pmRFn2F"
      },
      "source": [
        "В pymorphy2 для морфологического анализа слов есть ```MorphAnalyzer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4nRuUu2Wg_g"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd6KdqzWg_h"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6hdm1KBFx18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1a42ec-bf42-4fb1-f2d3-f43e96bf9bb4"
      },
      "source": [
        "sent = ['Если', 'б', 'мне', 'платили', 'каждый', 'раз']\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr1C2beNF3vE"
      },
      "source": [
        "Лемматизируем слово \"платили\" из предложения ```sent``` с помощью метода ```parse()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3zNlPBWg_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda2cc39-d942-4c11-9a5b-6737242a6390"
      },
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[3])\n",
        "ana"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2O2BL4_GJzq"
      },
      "source": [
        "Выведем его нормальную форму:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zp0KZLWg_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5deda388-0557-4d68-ff17-9fcc8e745f95"
      },
      "source": [
        "ana[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'платить'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hedBdcYWhAH"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом - главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhZMwsY5WhAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3b76a5-ebd3-42e7-89bb-9f0d8fc919b1"
      },
      "source": [
        "# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!\n",
        "alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
        "                                               tokenizer=word_tokenize)\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(alternative_tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(alternative_tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     28023\n",
            "    positive       1.00      1.00      1.00     28686\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSFebp_1WhAK"
      },
      "source": [
        "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQbCtidWhAP"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhUG9qWuWhAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822c7276-1197-4dfa-ffa8-cb90e127d159"
      },
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32943\n",
            "    positive       0.83      1.00      0.91     23766\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.91      0.93      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrqW55jgWhAR"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lNhEmyWhAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9263dc-0183-4b30-e093-745b93441427"
      },
      "source": [
        "# инициализируем векторайзер для символов\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.99      1.00     28023\n",
            "    positive       0.99      1.00      1.00     28686\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLMMicsFWhAY"
      },
      "source": [
        "Из предыдущего раздела уже понятно, почему на этих данных точность равна 1. \n",
        "\n",
        "Символьные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов - для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Бонус***: регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909c82c2-3590-4cd7-de11-61b1206d54bb"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2d7380-9f5f-4d51-821d-0e434da26b05"
      },
      "source": [
        "sent = 'hello world, I am Alina'.split()\n",
        "for s in sent:\n",
        "  print(re.findall('\\w{2}',s))\n",
        "#result=re.findall('\\w{2}','hello world')\n",
        "#result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['he', 'll']\n",
            "['wo', 'rl']\n",
            "[]\n",
            "['am']\n",
            "['Al', 'in']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79c7c00-f57c-43f1-e546-0b72bb0be900"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cea2b27-2e21-4c93-ac19-c94f930df793"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7e6c80-1dbb-42de-9124-61fa5d20d4ba"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e270397-7378-4b47-8346-126fcfdde823"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZ5qn3DWhA3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtlJMt0YWhA6"
      },
      "source": [
        "Если всё ещё осталось время: [регулярочный кроссворд ¯\\_(ツ)_/¯](https://mariolurig.com/crossword/)"
      ]
    }
  ]
}